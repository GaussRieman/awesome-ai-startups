#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ›ä¸šå…¬å¸åˆ†æå™¨æ ¸å¿ƒé€»è¾‘
ä½¿ç”¨æ–°çš„æç¤ºè¯æ¨¡æ¿è¿›è¡Œä¸€æ¬¡æ€§åˆ†æï¼Œå¹¶è§£æä¸ºç»“æ„åŒ–æ•°æ®
"""

import re
from datetime import datetime
from typing import Dict, Any, List, Tuple
from models import StartupAnalysis
from llm_client import LLMClient


class StartupAnalyzer:
    """åˆ›ä¸šå…¬å¸åˆ†æå™¨"""
    
    def __init__(self, llm_client: LLMClient):
        self.llm = llm_client
    
    def _extract_key_elements(self, text: str, llm_response: str) -> Dict[str, Any]:
        """ä»LLMå“åº”ä¸­æå–å…³é”®è¦ç´ """
        elements = {}
        
        # æå–å…¬å¸åç§°
        name_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', text[:200])
        if name_match:
            elements['company_name'] = name_match.group(1)
        
        # æå–æˆç«‹æ—¶é—´
        founded_match = re.search(r'([0-9]{4}[-/å¹´][0-9]{1,2}[-/æœˆ][0-9]{1,2}[æ—¥]?)', llm_response)
        if founded_match:
            elements['founded'] = founded_match.group(1)
        
        # æå–è¡Œä¸š/é¢†åŸŸ
        sector_match = re.search(r'è¡Œä¸š[ï¼š:]\s*([^\n]+)', llm_response)
        if sector_match:
            elements['sector'] = sector_match.group(1).strip()
        
        # æå–ä»·å€¼ä¸»å¼ 
        value_match = re.search(r'ä»·å€¼ä¸»å¼ [ï¼š:]\s*([^\n]+)', llm_response)
        if value_match:
            elements['one_liner'] = value_match.group(1).strip()
        
        return elements
    
    def _extract_keywords(self, text: str, llm_response: str) -> List[Tuple[str, float]]:
        """æå–å…³é”®è¯å’Œæƒé‡"""
        all_text = text + " " + llm_response
        
        # ä¸­æ–‡å…³é”®è¯
        chinese_words = re.findall(r'[\u4e00-\u9fa5]{2,}', all_text)
        # è‹±æ–‡å…³é”®è¯
        english_words = re.findall(r'\b[A-Za-z]{3,}\b', all_text)
        
        # åœç”¨è¯
        stop_words = {'å…¬å¸', 'æˆ‘ä»¬', 'ä»¥åŠ', 'ä¸€ä¸ª', 'çš„', 'å’Œ', 'æ˜¯', 'åœ¨', 'å¯¹', 'ä¸', 'åŠ', 'ç­‰'}
        
        # ç»Ÿè®¡è¯é¢‘
        word_freq = {}
        for word in chinese_words + english_words:
            if word not in stop_words and len(word) > 1:
                word_freq[word] = word_freq.get(word, 0) + 1
        
        # è½¬æ¢ä¸ºæƒé‡
        if word_freq:
            max_freq = max(word_freq.values())
            keywords = [(word, freq / max_freq) for word, freq in word_freq.items()]
            keywords.sort(key=lambda x: x[1], reverse=True)
            return keywords[:20]
        
        return []
    
    def _build_graph(self, text: str, llm_response: str) -> Dict[str, Any]:
        """æ„å»ºç®€å•çš„çŸ¥è¯†å›¾è°±"""
        graph = {"nodes": [], "edges": []}
        
        # æå–å…¬å¸åç§°
        company_name = None
        name_match = re.search(r'([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)', text[:200])
        if name_match:
            company_name = name_match.group(1)
        
        if company_name:
            # æ·»åŠ å…¬å¸èŠ‚ç‚¹
            graph["nodes"].append({
                "id": f"company:{company_name.lower()}",
                "label": company_name,
                "type": "Company"
            })
            
            # å°è¯•æå–åˆ›å§‹äºº
            founder_match = re.search(r'åˆ›å§‹äºº[ï¼š:]\s*([^\n]+)', llm_response)
            if founder_match:
                founder_name = founder_match.group(1).strip()
                graph["nodes"].append({
                    "id": f"person:{founder_name.lower()}",
                    "label": founder_name,
                    "type": "Person"
                })
                graph["edges"].append({
                    "source": f"company:{company_name.lower()}",
                    "target": f"person:{founder_name.lower()}",
                    "rel": "FOUNDED_BY"
                })
        
        return graph
    
    def analyze_startup(self, text: str, stream: bool = True) -> StartupAnalysis:
        """å®Œæ•´çš„åˆ›ä¸šå…¬å¸åˆ†ææµç¨‹"""
        print("ğŸ” å¼€å§‹åˆ†æåˆ›ä¸šå…¬å¸...")
        
        try:
            # ä½¿ç”¨æ–°çš„æç¤ºè¯æ¨¡æ¿è¿›è¡Œä¸€æ¬¡æ€§åˆ†æï¼Œæ”¯æŒæµå¼è¾“å‡º
            response = self.llm.call_llm(text, stream=stream)
            
            # è§£æå“åº”ï¼Œæ„å»ºç»“æ„åŒ–æ•°æ®
            key_elements = self._extract_key_elements(text, response)
            keywords = self._extract_keywords(text, response)
            graph = self._build_graph(text, response)
            
            # åˆ›å»ºåˆ†æç»“æœ
            analysis = StartupAnalysis(
                extracted_at=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                source_text=text[:500] + "..." if len(text) > 500 else text,
                raw_response=response,
                key_elements=key_elements,
                graph=graph,
                keywords=keywords,
                sources=[{
                    "title": "åŸå§‹æ–‡æœ¬åˆ†æ",
                    "url": "N/A",
                    "level": "L1",
                    "captured_at": datetime.now().strftime("%Y-%m-%d")
                }]
            )
            
            return analysis
            
        except Exception as e:
            raise Exception(f"åˆ†æå¤±è´¥: {str(e)}") 